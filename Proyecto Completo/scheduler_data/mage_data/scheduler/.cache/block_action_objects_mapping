{"block_file": {"data_exporters/data_exporter_poryecto2.py:data_exporter:python:data exporter poryecto2": {"content": "from mage_ai.io.snowflake import Snowflake\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data' in globals():\n    config_path = 'io_config.yaml'\n    config_profile = 'default'\n\n    Snowflake.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        data['yellow_trips_2020_01'],\n        table_name='yellow_trips_bronze',\n        schema_name='BRONZE',\n        database_name=get_secret_value('SNOWFLAKE_DATABASE'),\n        if_exists='replace',   # en pruebas, luego usar append/upsert\n    )\n", "file_path": "data_exporters/data_exporter_poryecto2.py", "language": "python", "type": "data_exporter", "uuid": "data_exporter_poryecto2"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_loaders/block_dl_proyecto2.py:data_loader:python:block dl proyecto2": {"content": "import pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport math\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    year, month, service_type = 2019, 1, \"yellow\"\n    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{service_type}_tripdata_{year}-{month:02d}.parquet\"\n\n    df = pd.read_parquet(url)\n    df['LOAD_YEAR'] = year\n    df['LOAD_MONTH'] = month\n    df['LOAD_SERVICE_TYPE'] = service_type\n\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_WAREHOUSE\"),\n        database=\"NYC_TAXI_DM\",\n        schema=\"BRONZE\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n    )\n\n    cur = conn.cursor()\n    cur.execute(f\"DROP TABLE IF EXISTS BRONZE.{service_type.upper()}_TRIPS_BRONZE\")\n\n    # \u26a1 Subir en chunks de 100k filas\n    chunk_size = 100_000\n    total_chunks = math.ceil(len(df) / chunk_size)\n\n    for i in range(total_chunks):\n        chunk = df.iloc[i*chunk_size:(i+1)*chunk_size]\n        print(f\"\u2b06\ufe0f Subiendo chunk {i+1}/{total_chunks} con {len(chunk)} filas...\")\n        write_pandas(conn, chunk, f\"{service_type.upper()}_TRIPS_BRONZE\", auto_create_table=True)\n\n    conn.close()\n    print(\"\u2705 Carga completada\")\n    return df.head(100)  # devolver solo preview\n", "file_path": "data_loaders/block_dl_proyecto2.py", "language": "python", "type": "data_loader", "uuid": "block_dl_proyecto2"}, "data_loaders/codigos_test.py:data_loader:python:codigos test": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nprint(\"ACCOUNT:\", get_secret_value(\"SNOWFLAKE_ACCOUNT\"))\nprint(\"USER:\", get_secret_value(\"SNOWFLAKE_USER\"))\nprint(\"ROLE:\", get_secret_value(\"SNOWFLAKE_ROLE\"))\n", "file_path": "data_loaders/codigos_test.py", "language": "python", "type": "data_loader", "uuid": "codigos_test"}, "data_loaders/data_loader_green_trips.py:data_loader:python:data loader green trips": {"content": "import pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport re\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Carga los datos del servicio GREEN TAXI (enero 2019) en Snowflake -> esquema BRONZE.\n    Limpia columnas conflictivas y a\u00f1ade metadatos despu\u00e9s de la carga.\n    \"\"\"\n\n    year, month, service_type = 2019, 1, \"green\"\n    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{service_type}_tripdata_{year}-{month:02d}.parquet\"\n\n    print(f\"\ud83d\udce6 Cargando datos desde: {url}\")\n    df = pd.read_parquet(url)\n\n    # Normalizar nombres de columnas\n    df.columns = [re.sub(r'[^0-9A-Z_]', '_', c.upper()) for c in df.columns]\n    df = df.loc[:, ~df.columns.duplicated()]\n    df = df[[c for c in df.columns if c.strip() != '']]\n\n    # Quitar columnas problem\u00e1ticas\n    invalid_cols = [\n        'EHAIL_FEE', 'TRIP_TYPE', 'ACCESS_A_RIDE_FLAG',\n        'SHARED_REQUEST_FLAG', 'SHARED_MATCH_FLAG'\n    ]\n    for col in invalid_cols:\n        if col in df.columns:\n            print(f\"\u26a0\ufe0f  Eliminando columna no soportada: {col}\")\n            df.drop(columns=[col], inplace=True)\n\n    # Convertir fechas\n    for col in df.columns:\n        if \"DATETIME\" in col.upper() or \"DATE\" in col.upper():\n            try:\n                df[col] = pd.to_datetime(df[col])\n            except Exception:\n                pass\n\n    print(f\"\u2705 Registros listos para subir: {len(df)}\")\n\n    # Conexi\u00f3n a Snowflake\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_WAREHOUSE\"),\n        database=\"NYC_TAXI_DM\",\n        schema=\"BRONZE\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n    )\n\n    table_name = f\"{service_type.upper()}_TRIPS_BRONZE\"\n\n    print(\"\ud83d\ude80 Subiendo datos a Snowflake (sin columnas de control)...\")\n\n    write_pandas(\n        conn,\n        df,\n        table_name,\n        auto_create_table=True,\n        overwrite=True\n    )\n\n    print(\"\u2705 Datos subidos correctamente.\")\n\n    # A\u00f1adir columnas de control directamente en Snowflake\n    with conn.cursor() as cur:\n        print(\"\ud83e\udde9 Verificando columnas de control en Snowflake...\")\n\n        # \u2705 Quitar \"IF NOT EXISTS\" despu\u00e9s de ALTER TABLE\n        cur.execute(f\"\"\"\n            ALTER TABLE {table_name}\n            ADD IF NOT EXISTS LOAD_YEAR INT,\n                            LOAD_MONTH INT,\n                            LOAD_SERVICE_TYPE STRING;\n        \"\"\")\n\n        cur.execute(f\"\"\"\n            UPDATE {table_name}\n            SET LOAD_YEAR = {year},\n                LOAD_MONTH = {month},\n                LOAD_SERVICE_TYPE = '{service_type}'\n            WHERE LOAD_YEAR IS NULL;\n        \"\"\")\n\n    # Registrar en coverage matrix\n    coverage = pd.DataFrame([{\n        \"LOAD_SERVICE_TYPE\": service_type,\n        \"LOAD_YEAR\": year,\n        \"LOAD_MONTH\": month,\n        \"FILE_EXISTS\": True,\n        \"LOAD_STATUS\": \"OK\"\n    }])\n    coverage.columns = [c.upper() for c in coverage.columns]\n    write_pandas(conn, coverage, \"COVERAGE_MATRIX\", auto_create_table=True, overwrite=False)\n\n    print(\"\ud83d\udcca Coverage Matrix actualizada.\")\n    conn.close()\n    print(\"\ud83d\udd12 Conexi\u00f3n cerrada correctamente.\")\n    return df\n", "file_path": "data_loaders/data_loader_green_trips.py", "language": "python", "type": "data_loader", "uuid": "data_loader_green_trips"}, "data_loaders/data_loader_taxis.py:data_loader:python:data loader taxis": {"content": "import pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport re\nimport requests\nimport io\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef load_data(**kwargs):\n    \"\"\"\n    Carga archivos Parquet de NYC Taxi en Snowflake (schema BRONZE).\n    Si el trigger no pasa argumentos, usa valores por defecto.\n    \"\"\"\n\n    # \u2705 Leer par\u00e1metros din\u00e1micos del trigger o usar valores por defecto\n    service_type = kwargs.get('service_type', 'yellow')\n    year = int(kwargs.get('year', 2025))\n\n    print(f\"\ud83d\ude80 Iniciando carga para {service_type.upper()} {year}...\")\n\n    # \ud83d\udd10 Credenciales Snowflake desde Mage Secrets\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_WAREHOUSE\"),\n        database=\"NYC_TAXI_DM\",\n        schema=\"BRONZE\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n    )\n\n    all_coverage = []\n\n    for month in range(1, 7):\n    #for month in [10, 11, 12]:\n        url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{service_type}_tripdata_{year}-{month:02d}.parquet\"\n        print(f\"\ud83d\udce6 Procesando {url} ...\")\n\n        try:\n            response = requests.get(url, timeout=300)\n            response.raise_for_status()\n\n            df = pd.read_parquet(io.BytesIO(response.content))\n            df.columns = [re.sub(r'[^0-9A-Z_]', '_', c.upper()) for c in df.columns]\n\n            df[\"LOAD_YEAR\"] = year\n            df[\"LOAD_MONTH\"] = month\n            df[\"LOAD_SERVICE_TYPE\"] = service_type\n\n            write_pandas(\n                conn,\n                df,\n                f\"{service_type.upper()}_TRIPS_BRONZE\",\n                auto_create_table=True,\n                overwrite=False\n            )\n\n            print(f\"\u2705 {service_type}_{year}-{month:02d} cargado ({len(df)} filas)\")\n\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": True,\n                \"LOAD_STATUS\": \"OK\"\n            })\n\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error con {url}: {e}\")\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": False,\n                \"LOAD_STATUS\": \"ERROR\"\n            })\n\n    coverage_df = pd.DataFrame(all_coverage)\n    coverage_df.columns = [c.upper() for c in coverage_df.columns]\n    write_pandas(conn, coverage_df, \"COVERAGE_MATRIX\", auto_create_table=True, overwrite=False)\n\n    conn.close()\n    print(f\"\ud83c\udfaf Carga finalizada para {service_type.upper()} {year}\")\n    return coverage_df\n", "file_path": "data_loaders/data_loader_taxis.py", "language": "python", "type": "data_loader", "uuid": "data_loader_taxis"}, "data_loaders/data_loader_taxis_green.py:data_loader:python:data loader taxis green": {"content": "import pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport re\nimport requests\nimport io\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef load_data(**kwargs):\n    \"\"\"\n    Carga archivos Parquet del servicio GREEN TAXI en Snowflake (schema BRONZE).\n    Descarga todos los meses del a\u00f1o indicado y registra cobertura.\n    \"\"\"\n\n    # \u2705 Par\u00e1metros din\u00e1micos o valores por defecto\n    service_type = kwargs.get('service_type', 'green')\n    year = int(kwargs.get('year', 2018))\n\n    print(f\"\ud83d\ude80 Iniciando carga para {service_type.upper()} {year}...\")\n\n    # \ud83d\udd10 Credenciales Snowflake desde Mage Secrets\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_WAREHOUSE\"),\n        database=\"NYC_TAXI_DM\",\n        schema=\"BRONZE\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n    )\n\n    all_coverage = []\n    table_name = f\"{service_type.upper()}_TRIPS_BRONZE\"\n\n    # \ud83d\udd01 Recorrer los meses del a\u00f1o\n    for month in range(1, 13):\n        url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{service_type}_tripdata_{year}-{month:02d}.parquet\"\n        print(f\"\\n\ud83d\udce6 Procesando {url} ...\")\n\n        try:\n            # Descargar Parquet\n            response = requests.get(url, timeout=60)\n            response.raise_for_status()\n\n            # Leer en DataFrame\n            df = pd.read_parquet(io.BytesIO(response.content))\n\n            # Normalizar nombres de columnas\n            df.columns = [re.sub(r'[^0-9A-Z_]', '_', c.upper()) for c in df.columns]\n            df = df.loc[:, ~df.columns.duplicated()]\n            df = df[[c for c in df.columns if c.strip() != '']]\n\n            # Eliminar columnas problem\u00e1ticas si existen\n            invalid_cols = [\n                'EHAIL_FEE', 'TRIP_TYPE', 'ACCESS_A_RIDE_FLAG',\n                'SHARED_REQUEST_FLAG', 'SHARED_MATCH_FLAG'\n            ]\n            for col in invalid_cols:\n                if col in df.columns:\n                    print(f\"\u26a0\ufe0f  Eliminando columna no soportada: {col}\")\n                    df.drop(columns=[col], inplace=True)\n\n            # Convertir columnas de fecha/hora si es posible\n            for col in df.columns:\n                if \"DATETIME\" in col.upper() or \"DATE\" in col.upper():\n                    try:\n                        df[col] = pd.to_datetime(df[col])\n                    except Exception:\n                        pass\n\n            # Agregar metadatos\n            df[\"LOAD_YEAR\"] = year\n            df[\"LOAD_MONTH\"] = month\n            df[\"LOAD_SERVICE_TYPE\"] = service_type\n\n            # Cargar en Snowflake\n            write_pandas(\n                conn,\n                df,\n                table_name,\n                auto_create_table=True,\n                overwrite=False  # idempotencia\n            )\n\n            print(f\"\u2705 {service_type}_{year}-{month:02d} cargado ({len(df)} filas)\")\n\n            # Registrar \u00e9xito\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": True,\n                \"LOAD_STATUS\": \"OK\"\n            })\n\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error con {url}: {e}\")\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": False,\n                \"LOAD_STATUS\": \"ERROR\"\n            })\n\n    # \ud83e\uddfe Registrar matriz de cobertura\n    coverage_df = pd.DataFrame(all_coverage)\n    coverage_df.columns = [c.upper() for c in coverage_df.columns]\n    write_pandas(conn, coverage_df, \"COVERAGE_MATRIX\", auto_create_table=True, overwrite=False)\n\n    conn.close()\n    print(f\"\\n\ud83c\udfaf Carga finalizada para {service_type.upper()} {year}\")\n    return coverage_df\n", "file_path": "data_loaders/data_loader_taxis_green.py", "language": "python", "type": "data_loader", "uuid": "data_loader_taxis_green"}, "data_loaders/data_loader_yellow_2025.py:data_loader:python:data loader yellow 2025": {"content": "import pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport re\nimport requests\nimport io\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef load_data(**kwargs):\n    \"\"\"\n    Carga archivos Parquet del NYC Taxi (yellow o green) en Snowflake (schema BRONZE)\n    con control autom\u00e1tico de columnas y registro de cobertura.\n    \"\"\"\n\n    # \u2699\ufe0f Par\u00e1metros din\u00e1micos (desde Trigger o valores por defecto)\n    service_type = kwargs.get('service_type', 'yellow')\n    year = int(kwargs.get('year', 2025))\n\n    print(f\"\ud83d\ude80 Iniciando carga para {service_type.upper()} {year}...\")\n\n    # \ud83d\udd10 Credenciales desde Mage Secrets\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_WAREHOUSE\"),\n        database=\"NYC_TAXI_DM\",\n        schema=\"BRONZE\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n    )\n\n    all_coverage = []\n    table_name = f\"{service_type.upper()}_TRIPS_BRONZE\"\n\n    # \ud83e\udde0 Intentar obtener columnas existentes (si la tabla ya fue creada)\n    try:\n        cur = conn.cursor()\n        cur.execute(f\"DESC TABLE IF EXISTS {table_name}\")\n        existing_cols = [row[0].upper() for row in cur.fetchall()]\n        print(f\"\ud83d\udccb Columnas existentes en {table_name}: {len(existing_cols)}\")\n    except Exception:\n        existing_cols = []\n        print(f\"\u2139\ufe0f Tabla {table_name} no existe a\u00fan, se crear\u00e1 autom\u00e1ticamente.\")\n\n    # \ud83d\udd01 Recorrer meses disponibles\n    for month in range(1, 13):\n        url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{service_type}_tripdata_{year}-{month:02d}.parquet\"\n        print(f\"\\n\ud83d\udce6 Procesando {url} ...\")\n\n        try:\n            response = requests.get(url, timeout=90)\n            if response.status_code != 200:\n                raise Exception(f\"Archivo no disponible ({response.status_code})\")\n\n            df = pd.read_parquet(io.BytesIO(response.content))\n\n            # \ud83e\uddf9 Normalizar nombres\n            df.columns = [re.sub(r'[^0-9A-Z_]', '_', c.upper()) for c in df.columns]\n            df = df.loc[:, ~df.columns.duplicated()]  # eliminar duplicadas\n            df = df[[c for c in df.columns if c.strip() != '']]  # eliminar vac\u00edas\n\n            # \u26a0\ufe0f Detectar columnas nuevas\n            new_cols = [c for c in df.columns if c not in existing_cols]\n            if new_cols:\n                print(f\"\u26a0\ufe0f Nuevas columnas detectadas ({len(new_cols)}): {new_cols}\")\n\n            # \ud83e\udde9 Filtrar solo columnas conocidas si la tabla ya existe\n            if existing_cols:\n                keep_cols = [c for c in df.columns if c in existing_cols or c.startswith(\"LOAD_\")]\n                df = df[keep_cols]\n\n            # \ud83d\udd52 Convertir campos de fecha/hora si aplican\n            for col in df.columns:\n                if \"DATE\" in col or \"TIME\" in col:\n                    try:\n                        df[col] = pd.to_datetime(df[col])\n                    except Exception:\n                        pass\n\n            # \ud83d\udcc5 A\u00f1adir metadatos\n            df[\"LOAD_YEAR\"] = year\n            df[\"LOAD_MONTH\"] = month\n            df[\"LOAD_SERVICE_TYPE\"] = service_type\n\n            # \ud83d\udcbe Cargar a Snowflake\n            write_pandas(\n                conn,\n                df,\n                table_name,\n                auto_create_table=True,\n                overwrite=False\n            )\n\n            print(f\"\u2705 {service_type}_{year}-{month:02d} cargado ({len(df)} filas)\")\n\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": True,\n                \"LOAD_STATUS\": \"OK\"\n            })\n\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error con {url}: {e}\")\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": False,\n                \"LOAD_STATUS\": \"ERROR\"\n            })\n\n    # \ud83e\uddfe Registrar cobertura\n    coverage_df = pd.DataFrame(all_coverage)\n    coverage_df.columns = [c.upper() for c in coverage_df.columns]\n    write_pandas(conn, coverage_df, \"COVERAGE_MATRIX\", auto_create_table=True, overwrite=False)\n\n    conn.close()\n    print(f\"\\n\ud83c\udfaf Carga completada para {service_type.upper()} {year}\")\n    return coverage_df\n", "file_path": "data_loaders/data_loader_yellow_2025.py", "language": "python", "type": "data_loader", "uuid": "data_loader_yellow_2025"}, "data_loaders/data_loader_yellow_trips.py:data_loader:python:data loader yellow trips": {"content": "import pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport re\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    # Par\u00e1metros de ingesta (puedes parametrizar con kwargs si quieres m\u00e1s adelante)\n    year, month, service_type = 2019, 1, \"yellow\"\n    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{service_type}_tripdata_{year}-{month:02d}.parquet\"\n\n    # Leer parquet desde la URL\n    df = pd.read_parquet(url)\n\n    # Forzar conversi\u00f3n de columnas datetime\n    for col in df.columns:\n        if \"DATETIME\" in col.upper() or \"DATE\" in col.upper():\n            try:\n                df[col] = pd.to_datetime(df[col])\n            except Exception:\n                pass\n\n    # Agregar columnas de control\n    df['LOAD_YEAR'] = year\n    df['LOAD_MONTH'] = month\n    df['LOAD_SERVICE_TYPE'] = service_type\n\n    # Normalizar nombres de columnas\n    df.columns = [re.sub(r'[^0-9A-Z_]', '_', c.upper()) for c in df.columns]\n\n    # Conectar a Snowflake\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_WAREHOUSE\"),\n        database=\"NYC_TAXI_DM\",\n        schema=\"BRONZE\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n    )\n\n    # Variable para definir si es primera carga o no\n    first_load = False   # Esta valor es True si es la primera vez para crear la tabla\n\n    # Subir datos de viajes\n    write_pandas(\n        conn,\n        df,\n        f\"{service_type.upper()}_TRIPS_BRONZE\",\n        auto_create_table=True,\n        overwrite=first_load\n    )\n\n    # Crear coverage matrix\n    coverage = pd.DataFrame([{\n        \"LOAD_SERVICE_TYPE\": service_type,\n        \"LOAD_YEAR\": year,\n        \"LOAD_MONTH\": month,\n        \"FILE_EXISTS\": True,\n        \"LOAD_STATUS\": \"OK\"\n    }])\n    coverage.columns = [c.upper() for c in coverage.columns]\n\n    write_pandas(\n        conn,\n        coverage,\n        \"COVERAGE_MATRIX\",\n        auto_create_table=True,\n        overwrite=first_load\n    )\n\n    conn.close()\n    return df\n", "file_path": "data_loaders/data_loader_yellow_trips.py", "language": "python", "type": "data_loader", "uuid": "data_loader_yellow_trips"}, "data_loaders/dl_taxizones.py:data_loader:python:dl taxizones": {"content": "import pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport re\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    # URL del lookup oficial de zonas\n    url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n\n    # Leer CSV\n    df = pd.read_csv(url)\n\n    # Normalizar nombres de columnas\n    df.columns = [re.sub(r'[^0-9A-Z_]', '_', c.upper()) for c in df.columns]\n\n    # Conectar a Snowflake\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_WAREHOUSE\"),\n        database=\"NYC_TAXI_DM\",\n        schema=\"BRONZE\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n    )\n\n    # \ud83d\udea6 Switch para definir si es primera carga o no\n    first_load = False   # ponlo en True la primera vez\n\n    # Subir datos a tabla Bronze\n    write_pandas(\n        conn,\n        df,\n        \"TAXI_ZONES_BRONZE\",\n        auto_create_table=True,\n        overwrite=first_load\n    )\n\n    conn.close()\n    return df\n", "file_path": "data_loaders/dl_taxizones.py", "language": "python", "type": "data_loader", "uuid": "dl_taxizones"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/testssecrets.py:data_loader:python:testssecrets": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\n\nprint(\"ACCOUNT:\", get_secret_value(\"SNOWFLAKE_ACCOUNT\"))\nprint(\"USER:\", get_secret_value(\"SNOWFLAKE_USER\"))\nprint(\"DATABASE:\", get_secret_value(\"SNOWFLAKE_DATABASE\"))\nprint(\"WAREHOUSE:\", get_secret_value(\"SNOWFLAKE_WAREHOUSE\"))\nprint(\"SCHEMA:\", get_secret_value(\"SNOWFLAKE_SCHEMA\"))\n", "file_path": "data_loaders/testssecrets.py", "language": "python", "type": "data_loader", "uuid": "testssecrets"}, "data_loaders/test_codigos.py:data_loader:python:test codigos": {"content": "import snowflake.connector\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_data(*args, **kwargs):\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_WAREHOUSE\"),\n        database=\"NYC_TAXI_DM\",\n        schema=\"BRONZE\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n    )\n    print(\"\u2705 conectado\")\n    conn.close()\n", "file_path": "data_loaders/test_codigos.py", "language": "python", "type": "data_loader", "uuid": "test_codigos"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/proyecto2/metadata.yaml:pipeline:yaml:proyecto2/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/data_loader_yellow_trips.py\n    file_source:\n      path: data_loaders/data_loader_yellow_trips.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_loader_yellow_trips\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_loader_yellow_trips\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: test_codigos\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: test_codigos\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: DL_TaxiZones\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: dl_taxizones\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/data_loader_green_trips.py\n    file_source:\n      path: data_loaders/data_loader_green_trips.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_loader_green_trips\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_loader_green_trips\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-23 20:28:06.817085+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: Proyecto2\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: proyecto2\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/proyecto2/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "proyecto2/metadata"}, "pipelines/proyecto2/__init__.py:pipeline:python:proyecto2/  init  ": {"content": "", "file_path": "pipelines/proyecto2/__init__.py", "language": "python", "type": "pipeline", "uuid": "proyecto2/__init__"}, "pipelines/proyecto2_dataloader_2015_2025/metadata.yaml:pipeline:yaml:proyecto2 dataloader 2015 2025/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_loader_taxis\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_loader_taxis\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: data_loader_taxis_green\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: data_loader_taxis_green\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-10-05 18:08:57.323719+00:00'\ndata_integration: null\ndescription: Data Loader de todos los datos\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: proyecto2_dataloader_2015-2025\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: proyecto2_dataloader_2015_2025\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/proyecto2_dataloader_2015_2025/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "proyecto2_dataloader_2015_2025/metadata"}, "pipelines/proyecto2_dataloader_2015_2025/__init__.py:pipeline:python:proyecto2 dataloader 2015 2025/  init  ": {"content": "", "file_path": "pipelines/proyecto2_dataloader_2015_2025/__init__.py", "language": "python", "type": "pipeline", "uuid": "proyecto2_dataloader_2015_2025/__init__"}, "/home/src/scheduler/data_loaders/data_loader_taxis.py:data_loader:python:home/src/scheduler/data loaders/data loader taxis": {"content": "import pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport re\nimport requests\nimport io\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef load_data(**kwargs):\n    \"\"\"\n    Carga archivos Parquet de NYC Taxi en Snowflake (schema BRONZE).\n    Si el trigger no pasa argumentos, usa valores por defecto.\n    \"\"\"\n\n    # \u2705 Leer par\u00e1metros din\u00e1micos del trigger o usar valores por defecto\n    service_type = kwargs.get('service_type', 'yellow')\n    year = int(kwargs.get('year', 2025))\n\n    print(f\"\ud83d\ude80 Iniciando carga para {service_type.upper()} {year}...\")\n\n    # \ud83d\udd10 Credenciales Snowflake desde Mage Secrets\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_WAREHOUSE\"),\n        database=\"NYC_TAXI_DM\",\n        schema=\"BRONZE\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n    )\n\n    all_coverage = []\n\n    for month in range(1, 7):\n    # for month in [10, 11, 12]:\n        url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{service_type}_tripdata_{year}-{month:02d}.parquet\"\n        print(f\"\ud83d\udce6 Procesando {url} ...\")\n\n        try:\n            response = requests.get(url, timeout=300)\n            response.raise_for_status()\n\n            df = pd.read_parquet(io.BytesIO(response.content))\n            df.columns = [re.sub(r'[^0-9A-Z_]', '_', c.upper()) for c in df.columns]\n\n            df[\"LOAD_YEAR\"] = year\n            df[\"LOAD_MONTH\"] = month\n            df[\"LOAD_SERVICE_TYPE\"] = service_type\n\n            write_pandas(\n                conn,\n                df,\n                f\"{service_type.upper()}_TRIPS_BRONZE\",\n                auto_create_table=True,\n                overwrite=False\n            )\n\n            print(f\"\u2705 {service_type}_{year}-{month:02d} cargado ({len(df)} filas)\")\n\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": True,\n                \"LOAD_STATUS\": \"OK\"\n            })\n\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error con {url}: {e}\")\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": False,\n                \"LOAD_STATUS\": \"ERROR\"\n            })\n\n    coverage_df = pd.DataFrame(all_coverage)\n    coverage_df.columns = [c.upper() for c in coverage_df.columns]\n    write_pandas(conn, coverage_df, \"COVERAGE_MATRIX\", auto_create_table=True, overwrite=False)\n\n    conn.close()\n    print(f\"\ud83c\udfaf Carga finalizada para {service_type.upper()} {year}\")\n    return coverage_df\n", "file_path": "/home/src/scheduler/data_loaders/data_loader_taxis.py", "language": "python", "type": "data_loader", "uuid": "data_loader_taxis"}, "/home/src/scheduler/data_loaders/data_loader_yellow_2025.py:data_loader:python:home/src/scheduler/data loaders/data loader yellow 2025": {"content": "import pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport re\nimport requests\nimport io\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef load_data(**kwargs):\n    \"\"\"\n    Carga archivos Parquet de NYC Taxi en Snowflake (schema BRONZE).\n    Ignora columnas nuevas (como CBD_CONGESTION_FEE) pero no crea tablas nuevas.\n    \"\"\"\n\n    service_type = kwargs.get('service_type', 'yellow')\n    year = int(kwargs.get('year', 2025))\n\n    print(f\"\ud83d\ude80 Iniciando carga para {service_type.upper()} {year}...\")\n\n    # \ud83d\udd10 Conexi\u00f3n Snowflake\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_WAREHOUSE\"),\n        database=\"NYC_TAXI_DM\",\n        schema=\"BRONZE\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n    )\n\n    table_name = f\"{service_type.upper()}_TRIPS_BRONZE\"\n    all_coverage = []\n\n    # \u2705 Obtener columnas actuales en la tabla (sin crear nada)\n    try:\n        cur = conn.cursor()\n        cur.execute(f\"DESC TABLE {table_name}\")\n        existing_cols = [row[0].upper() for row in cur.fetchall()]\n        print(f\"\ud83d\udccb Columnas actuales en {table_name}: {len(existing_cols)} detectadas.\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f No se pudo leer estructura de {table_name}. Error: {e}\")\n        existing_cols = []\n\n    for month in range(1, 7):\n        url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{service_type}_tripdata_{year}-{month:02d}.parquet\"\n        print(f\"\\n\ud83d\udce6 Procesando {url} ...\")\n\n        try:\n            response = requests.get(url, timeout=300)\n            response.raise_for_status()\n            df = pd.read_parquet(io.BytesIO(response.content))\n\n            # Normalizar columnas\n            df.columns = [re.sub(r'[^0-9A-Z_]', '_', c.upper()) for c in df.columns]\n\n            # \u2705 Mantener solo columnas que ya existen en Snowflake\n            if existing_cols:\n                df = df[[c for c in df.columns if c in existing_cols or c in [\"LOAD_YEAR\", \"LOAD_MONTH\", \"LOAD_SERVICE_TYPE\"]]]\n            else:\n                print(\"\u26a0\ufe0f No se detectaron columnas existentes, se usar\u00e1 estructura completa del archivo.\")\n\n            # A\u00f1adir metadatos\n            df[\"LOAD_YEAR\"] = year\n            df[\"LOAD_MONTH\"] = month\n            df[\"LOAD_SERVICE_TYPE\"] = service_type\n\n            write_pandas(\n                conn,\n                df,\n                table_name,\n                auto_create_table=False,  # \ud83d\udd12 No crear tabla nueva\n                overwrite=False\n            )\n\n            print(f\"\u2705 {service_type}_{year}-{month:02d} cargado ({len(df)} filas)\")\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": True,\n                \"LOAD_STATUS\": \"OK\"\n            })\n\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error con {url}: {e}\")\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": False,\n                \"LOAD_STATUS\": \"ERROR\"\n            })\n\n    coverage_df = pd.DataFrame(all_coverage)\n    coverage_df.columns = [c.upper() for c in coverage_df.columns]\n    write_pandas(conn, coverage_df, \"COVERAGE_MATRIX\", auto_create_table=True, overwrite=False)\n\n    conn.close()\n    print(f\"\\n\ud83c\udfaf Carga finalizada para {service_type.upper()} {year}\")\n    return coverage_df\n", "file_path": "/home/src/scheduler/data_loaders/data_loader_yellow_2025.py", "language": "python", "type": "data_loader", "uuid": "data_loader_yellow_2025"}, "/home/src/scheduler/data_loaders/data_loader_green_2025.py:data_loader:python:home/src/scheduler/data loaders/data loader green 2025": {"content": "import pandas as pd\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport re\nimport requests\nimport io\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef load_data(**kwargs):\n    \"\"\"\n    Carga archivos Parquet del servicio GREEN TAXI en Snowflake (schema BRONZE).\n    Compatible con a\u00f1os recientes (p. ej. 2025) que incluyen nuevas columnas.\n    \"\"\"\n\n    service_type = kwargs.get('service_type', 'green')\n    year = int(kwargs.get('year', 2025))\n\n    print(f\"\ud83d\ude80 Iniciando carga para {service_type.upper()} {year}...\")\n\n    # ======================\n    # \ud83d\udd10 Conexi\u00f3n a Snowflake\n    # ======================\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_WAREHOUSE\"),\n        database=\"NYC_TAXI_DM\",\n        schema=\"BRONZE\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n    )\n\n    table_name = f\"{service_type.upper()}_TRIPS_BRONZE\"\n    all_coverage = []\n\n    # ======================\n    # \ud83d\udccb Obtener columnas existentes\n    # ======================\n    try:\n        cur = conn.cursor()\n        cur.execute(f\"DESC TABLE {table_name}\")\n        existing_cols = [row[0].upper() for row in cur.fetchall()]\n        print(f\"\ud83d\udccb Columnas actuales en {table_name}: {len(existing_cols)} detectadas.\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f No se pudo leer estructura de {table_name}. Error: {e}\")\n        existing_cols = []\n\n    # ======================\n    # \ud83d\udcc6 Cargar cada mes del a\u00f1o\n    # ======================\n    for month in range(1, 7):\n        url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{service_type}_tripdata_{year}-{month:02d}.parquet\"\n        print(f\"\\n\ud83d\udce6 Procesando {url} ...\")\n\n        try:\n            response = requests.get(url, timeout=120)\n            response.raise_for_status()\n            df = pd.read_parquet(io.BytesIO(response.content))\n\n            # Normalizar nombres de columnas\n            df.columns = [re.sub(r'[^0-9A-Z_]', '_', c.upper()) for c in df.columns]\n            df = df.loc[:, ~df.columns.duplicated()]\n            df = df[[c for c in df.columns if c.strip() != '']]\n\n            # Eliminar columnas problem\u00e1ticas si existen\n            invalid_cols = [\n                'EHAIL_FEE', 'TRIP_TYPE', 'ACCESS_A_RIDE_FLAG',\n                'SHARED_REQUEST_FLAG', 'SHARED_MATCH_FLAG'\n            ]\n            for col in invalid_cols:\n                if col in df.columns:\n                    print(f\"\u26a0\ufe0f  Eliminando columna no soportada: {col}\")\n                    df.drop(columns=[col], inplace=True)\n\n            # \u2699\ufe0f Mantener solo columnas v\u00e1lidas que existan en Snowflake\n            if existing_cols:\n                df = df[[c for c in df.columns if c in existing_cols or c in [\"LOAD_YEAR\", \"LOAD_MONTH\", \"LOAD_SERVICE_TYPE\"]]]\n            else:\n                print(\"\u26a0\ufe0f No se detectaron columnas existentes, se usar\u00e1 estructura completa del archivo.\")\n\n            # Convertir fechas a datetime (seguro)\n            for col in df.columns:\n                if \"DATETIME\" in col.upper() or \"DATE\" in col.upper():\n                    try:\n                        df[col] = pd.to_datetime(df[col])\n                    except Exception:\n                        pass\n\n            # Agregar columnas de control\n            df[\"LOAD_YEAR\"] = year\n            df[\"LOAD_MONTH\"] = month\n            df[\"LOAD_SERVICE_TYPE\"] = service_type\n\n            # Cargar en Snowflake\n            write_pandas(\n                conn,\n                df,\n                table_name,\n                auto_create_table=False,  # No crear nuevas tablas\n                overwrite=False            # Append seguro\n            )\n\n            print(f\"\u2705 {service_type}_{year}-{month:02d} cargado ({len(df)} filas)\")\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": True,\n                \"LOAD_STATUS\": \"OK\"\n            })\n\n        except Exception as e:\n            print(f\"\u26a0\ufe0f Error con {url}: {e}\")\n            all_coverage.append({\n                \"LOAD_SERVICE_TYPE\": service_type,\n                \"LOAD_YEAR\": year,\n                \"LOAD_MONTH\": month,\n                \"FILE_EXISTS\": False,\n                \"LOAD_STATUS\": \"ERROR\"\n            })\n\n    # ======================\n    # \ud83e\uddfe Registrar cobertura\n    # ======================\n    coverage_df = pd.DataFrame(all_coverage)\n    coverage_df.columns = [c.upper() for c in coverage_df.columns]\n    write_pandas(conn, coverage_df, \"COVERAGE_MATRIX\", auto_create_table=True, overwrite=False)\n\n    conn.close()\n    print(f\"\\n\ud83c\udfaf Carga finalizada para {service_type.upper()} {year}\")\n    return coverage_df\n", "file_path": "/home/src/scheduler/data_loaders/data_loader_green_2025.py", "language": "python", "type": "data_loader", "uuid": "data_loader_green_2025"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}
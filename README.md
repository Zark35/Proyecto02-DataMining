# Proyecto 02 Data Mining 
# Estudiante: AndrÃ©s BohÃ³rquez | CÃ³digo: 00320727

# **DescripciÃ³n y diagrama de arquitectura (bronze/silver/gold) y orquestaciÃ³n en Mage**
Este proyecto implementa un data pipeline moderno para el procesamiento de datos de los servicios Yellow Taxi y Green Taxi de la ciudad de Nueva York, desde los archivos pÃºblicos en formato Parquet (2015â€“2025), integrando:

* Mage como orquestador de ingesta y transformaciÃ³n.
* Snowflake como Data Warehouse principal (capas Bronze, Silver y Gold).
* dbt para modelado, pruebas de calidad y documentaciÃ³n de datos.

* Mage como orquestador de ingesta y transformaciÃ³n.  
* Snowflake como Data Warehouse principal (capas Bronze, Silver y Gold).  
* dbt para modelado, pruebas de calidad y documentaciÃ³n de datos.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Source (NYC) â”‚
â”‚  Parquet Files     â”‚
â”‚  (2015â€“2025)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Mage Data Loader (Python)     â”‚
â”‚  - Ingesta mensual por aÃ±o     â”‚
â”‚  - Control con COVERAGE_MATRIX â”‚
â”‚  - Capa BRONZE (raw)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dbt Models (SILVER)       â”‚
â”‚  - Limpieza / ValidaciÃ³n   â”‚
â”‚  - UniÃ³n de Green & Yellow â”‚
â”‚  - NormalizaciÃ³n de tipos  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dbt Models (GOLD)         â”‚
â”‚  - Hechos: fct_trips       â”‚
â”‚  - Dimensiones: date, zone â”‚
â”‚  - Ratecode, payment, etc. â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Snowflake Analytics Layer      â”‚
â”‚  - Clustering por fecha y zona  â”‚
â”‚  - Query tuning y pruning       â”‚
â”‚  - Calidad de datos documentada â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
OrquestaciÃ³n en Mage:
* Flujos programados por aÃ±o y servicio (yellow, green).
* EjecuciÃ³n secuencial con control de errores y idempotencia (evita duplicados).
* Cada carga mensual actualiza la tabla BRONZE.COVERAGE_MATRIX.

<br>

# **Cobertura de Datos**
Los datos fueron ingeridos desde enero de 2015 hasta junio de 2025, cubriendo ambos servicios:

```text
Servicio	| AÃ±os      | Cargados   |	Total Archivos | Estado
-----------------------------------------------------------------
Yellow Taxi	| 2015â€“2025 |	126	     | âœ…              | Completo
Green Taxi	| 2015â€“2025 |	126   	 | âœ…              | Completo
```

Para comprobar este apartado se ejecutan algunos querys en Snowflake, como:

USE DATABASE NYC_TAXI_DM;

USE SCHEMA BRONZE;

SELECT LOAD_SERVICE_TYPE, LOAD_YEAR, LOAD_MONTH, LOAD_STATUS

FROM COVERAGE_MATRIX

ORDER BY LOAD_SERVICE_TYPE, LOAD_YEAR, LOAD_MONTH;

![alt text](<Screen Cobertura Datos 2.png>)

Otro query de Snowflake para comprobar todos los datos fue:

USE DATABASE NYC_TAXI_DM;

USE SCHEMA BRONZE;

SELECT LOAD_SERVICE_TYPE, LOAD_YEAR, LOAD_MONTH, COUNT(*) AS total_filas

FROM YELLOW_TRIPS_BRONZE

GROUP BY LOAD_SERVICE_TYPE, LOAD_YEAR, LOAD_MONTH

ORDER BY LOAD_YEAR, LOAD_MONTH;

![alt text](<Screen Cobertura Datos 1.png>)

<br>

# **Estrategia de pipeline de backfill mensual e idempotencia.**
Para el backfill mi proceso fue tener varios bloques de cÃ³digo en Mage.

Por un lado, tengo un primer pipeline en el qe descarguÃ© todos los datos para enero de 2019 y con lo que empecÃ© a trabajar para ir desarrollando los archivos dbt y llegar a las pregunta. TambiÃ©n con un bloque para verificar la conexiÃ³n con snowflake y comprobar que se guarden en secrets los datos. Y un Ãºltimo bloque para el apartado de cargar las zonas. 

Por otro lado, tengo un segundo pipeline donde tengo cuatro bloques. Dos primeros enfocados en recopilar todos los datos desde 2015 hasta 2024, uno para yello y otro green. Al igual que en el anterior caso, dos bloque para yellow y green pero en este caso para el aÃ±o 2025.

Asimismo, en el caso de sobrar algunos bloques de data loaders, son referentes a pruebas que realicÃ© previamente y me olvidÃ© de eliminar. Los mÃ¡s fundamentales estÃ¡n integrados en los dos pipelines de proyecto 2.

La idempotencia la garantizo al momento de ejecutar cada bloque y al momento de presentarse algÃºn fallo, cÃ³mo se puede observar en la screenshot anterior de Snowflake, solo debo cambiar en el for de los meses el valor correspondiente al mes que dio problemas. En el caso de yellow en la fecha 2023-08 dio problemas debido a que se acabÃ³ el tiempo de conexiÃ³n. Se solucionÃ³ volviendo a correr el cÃ³digo en el rango de 8 y funcionÃ³. Asimismo, no existe problema de que se dupliquen los datos al volver a correr porque el cÃ³digo no sobreescribe.

<br>

# **GestiÃ³n de secretos y cuenta de servicio / rol**

Para este apartado, lo primero fue guardar en secrets los valores de los secretos correspondientes a la cuenta con permisos mÃ­nimos de Snowflake, no la cuenta administrador. 

Y la cuenta la cree inicialmente con un query en Snowflake similar al siguiente: 

```python
-- Crear rol de ingesta
CREATE OR REPLACE ROLE role_dm_ingest;

-- Crear usuario de servicio (ejemplo: svc_dm_ingest)
CREATE OR REPLACE USER svc_dm_ingest
  PASSWORD = 'ContraseÃ±aSegura123'
  DEFAULT_ROLE = role_dm_ingest
  DEFAULT_WAREHOUSE = wh_dm
  DEFAULT_NAMESPACE = nyc_taxi_dm.bronze
  MUST_CHANGE_PASSWORD = FALSE;

-- Permisos mÃ­nimos al rol
GRANT USAGE ON WAREHOUSE wh_dm TO ROLE role_dm_ingest;

GRANT USAGE ON DATABASE nyc_taxi_dm TO ROLE role_dm_ingest;
GRANT USAGE ON SCHEMA nyc_taxi_dm.bronze TO ROLE role_dm_ingest;
GRANT CREATE TABLE ON SCHEMA nyc_taxi_dm.bronze TO ROLE role_dm_ingest;
GRANT INSERT, SELECT, UPDATE, DELETE ON ALL TABLES IN SCHEMA nyc_taxi_dm.bronze TO ROLE role_dm_ingest;
GRANT INSERT, SELECT, UPDATE, DELETE ON FUTURE TABLES IN SCHEMA nyc_taxi_dm.bronze TO ROLE role_dm_ingest;

-- Asignar el rol al usuario
GRANT ROLE role_dm_ingest TO USER svc_dm_ingest;
```

<br>

# **DiseÃ±o de silver (reglas de limpieza/estandarizaciÃ³n) y gold (hechos/dimensiones)**

La capa Silver tiene como objetivo transformar los datos brutos del esquema Bronze en un formato coherente, estandarizado y sin valores errÃ³neos.
Esta etapa busca asegurar la consistencia semÃ¡ntica y estructural de todos los datasets antes de su anÃ¡lisis en la capa Gold.

Reglas aplicadas:

âœ… EstandarizaciÃ³n de nombres de columnas:
Todos los nombres fueron convertidos a mayÃºsculas con guion bajo, eliminando caracteres especiales (por ejemplo, tpep_pickup_datetime â†’ TPEP_PICKUP_DATETIME).

âœ… TipificaciÃ³n de campos:
ConversiÃ³n explÃ­cita de los tipos de datos a sus formatos correctos:

Fechas (pickup_datetime, dropoff_datetime) â†’ TIMESTAMP

Distancias â†’ FLOAT

Tarifas y montos â†’ NUMERIC(10,2)

Identificadores â†’ INTEGER

âœ… Tratamiento de valores nulos y duplicados:

EliminaciÃ³n de registros duplicados basados en (PICKUP_DATETIME, DROPOFF_DATETIME, VENDORID)

Reemplazo o eliminaciÃ³n de filas con trip_distance <= 0 o fare_amount < 0.

âœ… CorrecciÃ³n de rangos invÃ¡lidos:

Se filtraron viajes con duraciones fuera de 1 a 180 minutos (para evitar datos corruptos o de prueba).

Se eliminaron filas con velocidades calculadas mayores a 80 mph.

âœ… NormalizaciÃ³n de identificadores de zonas:
Se garantizÃ³ la correspondencia entre pickup_location_id y las zonas definidas en dim_zone, usando JOIN con la tabla de zonas de NYC.

âœ… UnificaciÃ³n temporal:
Se aÃ±adieron las columnas de control:
LOAD_YEAR, LOAD_MONTH, y LOAD_SERVICE_TYPE
para rastrear el origen y periodo de carga de cada dataset.

Resultado:
Los datos en Silver estÃ¡n limpios, consistentes, y listos para integrarse en un modelo analÃ­tico estable.

<br>

La capa Gold implementa un modelo tipo estrella (Star Schema), orientado al anÃ¡lisis de negocio, agregando los datos en una estructura de hechos y dimensiones.
El objetivo es optimizar la consulta de mÃ©tricas clave (hechos) asociadas a descripciones contextuales (dimensiones).

DiseÃ±o del modelo:

ğŸ“¦ Tabla de hechos:

FCT_TRIPS
Contiene las medidas cuantitativas del sistema de taxis.

Principales mÃ©tricas:

trip_distance

trip_duration_min (duraciÃ³n en minutos calculada como diferencia entre pickup y dropoff)

fare_amount, tip_amount, total_amount

congestion_surcharge, extra, tolls_amount

Columnas de control (load_year, load_month, service_type)

Claves forÃ¡neas:

pickup_zone_sk, dropoff_zone_sk â†’ DIM_ZONE

pickup_date_sk, dropoff_date_sk â†’ DIM_DATE

pickup_time_sk, dropoff_time_sk â†’ DIM_TIME

<br>

# **Clustering**

El proceso de clustering se aplicÃ³ sobre la tabla de hechos FCT_TRIPS con el fin de mejorar el rendimiento de las consultas analÃ­ticas realizadas en la capa GOLD, especialmente aquellas que filtran por fecha, zona y tipo de servicio.

Llaves elegidas:

ALTER TABLE SILVER_GOLD.FCT_TRIPS

CLUSTER BY (pickup_date_sk, pickup_zone_sk, service_type);

<br>

pickup_date_sk â†’ Permite optimizar las consultas temporales y cÃ¡lculos de tendencias (por aÃ±o/mes).

pickup_zone_sk â†’ Mejora la segmentaciÃ³n espacial, muy usada en anÃ¡lisis por borough o zonas de alta demanda.

service_type â†’ Distingue entre yellow y green taxis, frecuentemente filtrados en las consultas de ingresos, duraciÃ³n o velocidad.

MÃ©tricas observadas (antes y despuÃ©s del clustering):

Antes del clustering, las consultas que agrupaban o filtraban por zona y fecha requerÃ­an scans completos de la tabla, afectando el tiempo de respuesta. (Demoraba 1.6 segundos)

DespuÃ©s de aplicar CLUSTER BY, Snowflake reorganizÃ³ fÃ­sicamente los micro-particiones de la tabla, reduciendo la necesidad de leer datos irrelevantes. (Se demora 1.3 segundos)

Al ejecutar la bÃºsqueda, se observÃ³ una disminuciÃ³n en el tiempo de ejecuciÃ³n.

**Conclusiones:**

El clustering en pickup_date_sk, pickup_zone_sk y service_type mejorÃ³ la eficiencia de lectura de la tabla de hechos, especialmente en las consultas mÃ¡s comunes del modelo analÃ­tico.

Se logrÃ³ una reducciÃ³n significativa en el tiempo de consulta (menos particiones escaneadas y menor uso de recursos del warehouse).

La estructura de micro-particiones ahora se alinea con los ejes principales del anÃ¡lisis (temporal, espacial y por tipo de servicio), manteniendo un equilibrio entre granularidad y desempeÃ±o.

En general, el clustering contribuye a que la capa GOLD sea mÃ¡s escalable y rÃ¡pida, sin alterar la lÃ³gica del modelo de datos.

<br>

# **Pruebas**
Durante la construcciÃ³n del modelo Silverâ€“Gold, se ejecutaron pruebas automÃ¡ticas mediante dbt test para asegurar la calidad, integridad y consistencia de los datos transformados.
Estas pruebas verifican que las tablas cumplan con las reglas bÃ¡sicas de limpieza y relaciones, antes de ser utilizadas en los anÃ¡lisis de la capa Gold.

**Pruebas aplicadas**

Pruebas de unicidad (unique)
Validan que las claves primarias o surrogate keys sean Ãºnicas dentro de sus dimensiones.

Ejemplo: zone_sk en dim_zone o date_sk en dim_date no deben repetirse.

dbt test --select dim_zone --store-failures


Pruebas de no nulos (not_null)
Garantizan que los campos esenciales no contengan valores vacÃ­os o perdidos.

Ejemplo: en fct_trips, los campos pickup_zone_sk y pickup_date_sk deben tener siempre valores vÃ¡lidos.

tests:
  - not_null:
      column_name: pickup_zone_sk
  - not_null:
      column_name: pickup_date_sk


Pruebas de integridad referencial (relationships)
Aseguran que las claves forÃ¡neas de la tabla de hechos existan en las dimensiones correspondientes.

Ejemplo: todos los valores de pickup_zone_sk en fct_trips deben tener coincidencia en dim_zone.zone_sk.

tests:
  - relationships:
      from: fct_trips.pickup_zone_sk
      to: dim_zone.zone_sk


Pruebas de rangos y valores esperados (personalizadas)
Validan que las medidas cuantitativas estÃ©n dentro de los lÃ­mites definidos.

Ejemplo: los viajes deben tener una duraciÃ³n y distancia lÃ³gicas.

SELECT *
FROM fct_trips
WHERE trip_duration_min NOT BETWEEN 1 AND 180
   OR trip_distance <= 0
   OR total_amount < 0;

<br>

# **Trobleshouting**

Debido al tiempo, la pregunta 5 se basa Ãºnicamente con la capa gold cuando tenÃ­a Ãºnicamente los datos de enero de 2019.

Debido a que al momento de crear el archivo profiles.yml debo definir un schema, los schemas con los que trabajo a partir de SILVER y GOLD en Snowflake, debo utilizar con un prefijo "SILVER_", por ejemplo: USE SCHEMA SILVER_GOLD; al momento de hacer el clustering.


# Checklist de implementaciÃ³n

- [x] Cargados todos los meses 2015â€“2025 (Parquet) de Yellow y Green; matriz de cobertura en README. NYC.gov

- [x] Mage orquesta backfill mensual con idempotencia y metadatos por lote.

- [x] Bronze (raw) refleja fielmente el origen; Silver unifica/escaliza; Gold en estrella con fct_trips y dimensiones clave

- [x] Clustering aplicado a fct_trips con evidencia antes/despuÃ©s (Query Profile, pruning). Snowflake Docs

- [x] Secrets y cuenta de servicio con permisos mÃ­nimos (evidencias sin exponer valores).

- [x] Tests dbt (not_null, unique, accepted_values, relationships) pasan; docs y lineage generados.
 
- [x] Notebook con respuestas a las 5 preguntas de negocio desde gold.